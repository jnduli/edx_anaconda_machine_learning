Unsupervised methods learnt:
Unsupervised methods learnt:

FOr reducing features:
1. PCA - features with linear rlshps
2. Isomap - features with non linea rlshps

For clustering
1. K-means

Using Kmeans
    >>> from sklearn.cluster import KMeans
    >>> kmeans = KMeans(n_clusters=5)
>>> kmeans.fit(df)
    KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=10,
            n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
            verbose=0)

>>> labels = kmeans.predict(df)
    >>> centroids = kmeans.cluster_centers_

    Each has a .fit() method


    Supervised Learning
    Data has to be split, one portion for machine learning and the other for testing.

    code for doing so
    >>> from sklearn.cross_validation import train_test_split
    >>> data   = [0,1,2,3,4, 5,6,7,8,9]  # input dataframe samples
    >>> labels = [0,0,0,0,0, 1,1,1,1,1]  # the function we're training is " >4 "

>>> data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.5, random_state=7)

    >>> data_train
    [9, 7, 3, 6, 4]

    >>> label_train
    [1, 1, 0, 1, 0]

    >>> data_test
    [8, 5, 0, 2, 1]

    >>> label_test
    [1, 1, 0, 0, 0]

    TO test the results of machine learnging the following can be bone;

    from sklearn.metrics import accuracy_score

# Returns an array of predictions:
>>> predictions = my_model.predict(data_test) 
    >>> predictions
    [0, 0, 0, 1, 0]

# The actual answers:
    >>> label_train
    [1, 1, 0, 1, 0]

>>> accuracy_score(label_train, predictions)
    0.59999999999999998

>>> accuracy_score(label_train, predictions, normalize=False)
    3
    K Nearest Neighbor:
    Stores all the samples and in testing, finds the K nearest neighbors.
    Works only on numerical features. Bag of words can be used to numerize.
    Once neighbors are found, K-Neighbours looks at their classes and votes to select the best fit.
    Decision boundaries:can take different shapes
    the higher the K value the smoother and less jittery the decision surface

    Before starting out, separate labels from the data features:
    X = pd.read_csv('data.set', index_col=0)
# Immediately copy out the classification / label / class / answer column
    y = X['classification'].copy()
    X.drop(labels=['classification'], inplace=True, axis=1)

    The KNeighborsClassifier class constructor takes in a few arguments, most optional:

    n_neighbors: The number of neighbors to consider. Keep it odd when doing binary classification, particularly when you use uniform weighting.
    weights: How to count the votes from the neighbors; does everyone get an equal vote, a weighted vote, or something else?
    algorithm: You can select an optimization method for searching through your training data set to find the nearest neighbors.

    And here is the algorithm in action:

    >>> # From now on, you only train on a "portion" of your dset:
>>> X_train = pd.DataFrame([ [0], [1], [2], [3] ])
    >>> y_train = [0, 0, 1, 1]

    >>> from sklearn.neighbors import KNeighborsClassifier
    >>> model = KNeighborsClassifier(n_neighbors=3)
    >>> model.fit(X_train, y_train) 
KNeighborsClassifier(...)

    >>> # You can pass in a dframe or an ndarray
>>> model.predict([[1.1]])

>>> model.predict_proba([[0.9]])
    [[ 0.66666667  0.33333333]]

    Gotchas: Data nas to be measurable
    Sensitive to feature scaling
    Care shoudl be taken with overall class distribution eg. 30%A and 70%B might lead to preference for B


    Linear Regression
    It implements:
fit(), predict(), fit_predict(), score()
    Outputs include:
    intercept_: the scalar constant offset value
    coef_: array of weights


    To model a dataset using linear regression, use the following code:

    >>> from sklearn import linear_model
    >>> model = linear_model.LinearRegression()
>>> model.fit(X_train, y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)

    Data to be separated.
    x_train: singe dimensional array, shaped like [n_samples] where each sample correspondss to the single feature being fit. You can rum multiple linear regression models simultaneously on same dataset by passing in [n_samplse, n_features] array.

    y_train: it can be [n_samples] or [n_samples, n_targets]

    .score(): returns R2 coeff. that shows how good you model fits your dat        >>> # R2 Score
              >>> model.score(X_test, y_test)
              153.244939109

    >>> # Sum of Squared Distances
>>> np.sum(model.predict(X_test) - y_test) ** 2)
    5465.15

    R2 increases with more features, thus be careful with features chosen otherwise overfitting occurs.

    It can also work with categorical data, using dummy boolean columns.Increase no of targets, or training label dimension, and each calculation will have its offset stores in intercepts_ array, and coeff_attribute becomes array of arrays, one per target.(Multi-Output Linear Regression).
    Gotchas:
    works only on linear rlshps
    cant provide some values since it works on mean
    assumes variables are linearly independent


    SVC
    Supporot Vector Machines are a set of supervsed lean. algorithms that can be used for class., regre. and outlier detection.
    SVC- Support Vector Classifier
    Used when speed is required.
    SVC is not probailistic. Classification is calculated based off the geometry of your dataset.


    Usage:
    Important params are:
    kernel : type of kernel used. Default is radial basis function rbf. Others supported include: linear, poly, sigmoid, precomputeed.
    Of the many configurable parameters for SciKit-Learn's svm.SVC class, the most important three in order are:
    C: penalty parameter for the error term. Lower C valuew, smoother and more generalzed the decision boundary.
    gamma: inversely proportional to extent single training sample influence extends.

    To get started with SVC, import it as usual:

    >>> from sklearn.svm import SVC
    >>> model = SVC(kernel='linear')
>>> model.fit(X, y) 
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
            decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
            max_iter=-1, probability=False, random_state=None, shrinking=True,
            tol=0.001, verbose=False)

It has .fit(), .predict(), .score()
    .decision_function(x): calculates the distance of a set of samples to the devision boundary in high dimensional sapce where x is series of samples of form [n_samples, n_features]
                           Attributes include:
                           support_: array of indices belonging to selected support vectors
                           support_vectors_: actual samples chosen as support vectors
                           intercept_:constants of the decision function
                           dual_coef_: each support vectors contribution to the decision function, on per classification basis.


                           FOr reducing features:
                           1. PCA - features with linear rlshps
                           2. Isomap - features with non linea rlshps

                           For clustering
                           1. K-means

                           Using Kmeans
    >>> from sklearn.cluster import KMeans
    >>> kmeans = KMeans(n_clusters=5)
>>> kmeans.fit(df)
    Unsupervised methods learnt:

    FOr reducing features:
    1. PCA - features with linear rlshps
    2. Isomap - features with non linea rlshps

    For clustering
    1. K-means

    Using Kmeans
    >>> from sklearn.cluster import KMeans
    >>> kmeans = KMeans(n_clusters=5)
>>> kmeans.fit(df)
    KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=10,
            n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
            verbose=0)

>>> labels = kmeans.predict(df)
    >>> centroids = kmeans.cluster_centers_

    Each has a .fit() method


    Supervised Learning
    Data has to be split, one portion for machine learning and the other for testing.

    code for doing so
    >>> from sklearn.cross_validation import train_test_split
    >>> data   = [0,1,2,3,4, 5,6,7,8,9]  # input dataframe samples
    >>> labels = [0,0,0,0,0, 1,1,1,1,1]  # the function we're training is " >4 "

>>> data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.5, random_state=7)

    >>> data_train
    [9, 7, 3, 6, 4]

    >>> label_train
    [1, 1, 0, 1, 0]

    >>> data_test
    [8, 5, 0, 2, 1]

    >>> label_test
    [1, 1, 0, 0, 0]

    TO test the results of machine learnging the following can be bone;

    from sklearn.metrics import accuracy_score

# Returns an array of predictions:
>>> predictions = my_model.predict(data_test) 
    >>> predictions
    [0, 0, 0, 1, 0]

# The actual answers:
    >>> label_train
    [1, 1, 0, 1, 0]

>>> accuracy_score(label_train, predictions)
    0.59999999999999998

>>> accuracy_score(label_train, predictions, normalize=False)
    3
    K Nearest Neighbor:
    Stores all the samples and in testing, finds the K nearest neighbors.
    Works only on numerical features. Bag of words can be used to numerize.
    Once neighbors are found, K-Neighbours looks at their classes and votes to select the best fit.
    Decision boundaries:can take different shapes
    the higher the K value the smoother and less jittery the decision surface

    Before starting out, separate labels from the data features:
    X = pd.read_csv('data.set', index_col=0)
# Immediately copy out the classification / label / class / answer column
    y = X['classification'].copy()
    X.drop(labels=['classification'], inplace=True, axis=1)

    The KNeighborsClassifier class constructor takes in a few arguments, most optional:

    n_neighbors: The number of neighbors to consider. Keep it odd when doing binary classification, particularly when you use uniform weighting.
    weights: How to count the votes from the neighbors; does everyone get an equal vote, a weighted vote, or something else?
    algorithm: You can select an optimization method for searching through your training data set to find the nearest neighbors.

    And here is the algorithm in action:

    >>> # From now on, you only train on a "portion" of your dset:
>>> X_train = pd.DataFrame([ [0], [1], [2], [3] ])
    >>> y_train = [0, 0, 1, 1]

    >>> from sklearn.neighbors import KNeighborsClassifier
    >>> model = KNeighborsClassifier(n_neighbors=3)
    >>> model.fit(X_train, y_train) 
KNeighborsClassifier(...)

    >>> # You can pass in a dframe or an ndarray
>>> model.predict([[1.1]])

>>> model.predict_proba([[0.9]])
    [[ 0.66666667  0.33333333]]

    Gotchas: Data nas to be measurable
    Sensitive to feature scaling
    Care shoudl be taken with overall class distribution eg. 30%A and 70%B might lead to preference for B


    Linear Regression
    It implements:
fit(), predict(), fit_predict(), score()
    Outputs include:
    intercept_: the scalar constant offset value
    coef_: array of weights


    To model a dataset using linear regression, use the following code:

    >>> from sklearn import linear_model
    >>> model = linear_model.LinearRegression()
>>> model.fit(X_train, y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)

    Data to be separated.
    x_train: singe dimensional array, shaped like [n_samples] where each sample correspondss to the single feature being fit. You can rum multiple linear regression models simultaneously on same dataset by passing in [n_samplse, n_features] array.

    y_train: it can be [n_samples] or [n_samples, n_targets]

    .score(): returns R2 coeff. that shows how good you model fits your dat        >>> # R2 Score
              >>> model.score(X_test, y_test)
              153.244939109

    >>> # Sum of Squared Distances
>>> np.sum(model.predict(X_test) - y_test) ** 2)
    5465.15

    R2 increases with more features, thus be careful with features chosen otherwise overfitting occurs.

    It can also work with categorical data, using dummy boolean columns.Increase no of targets, or training label dimension, and each calculation will have its offset stores in intercepts_ array, and coeff_attribute becomes array of arrays, one per target.(Multi-Output Linear Regression).
    Gotchas:
    works only on linear rlshps
    cant provide some values since it works on mean
    assumes variables are linearly independent


    SVC
    Supporot Vector Machines are a set of supervsed lean. algorithms that can be used for class., regre. and outlier detection.
    SVC- Support Vector Classifier
    Used when speed is required.
    SVC is not probailistic. Classification is calculated based off the geometry of your dataset.


    Usage:
    Important params are:
    kernel : type of kernel used. Default is radial basis function rbf. Others supported include: linear, poly, sigmoid, precomputeed.
    Of the many configurable parameters for SciKit-Learn's svm.SVC class, the most important three in order are:
    C: penalty parameter for the error term. Lower C valuew, smoother and more generalzed the decision boundary.
    gamma: inversely proportional to extent single training sample influence extends.

    To get started with SVC, import it as usual:

    >>> from sklearn.svm import SVC
    >>> model = SVC(kernel='linear')
>>> model.fit(X, y) 
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
            decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
            max_iter=-1, probability=False, random_state=None, shrinking=True,
            tol=0.001, verbose=False)

It has .fit(), .predict(), .score()
    .decision_function(x): calculates the distance of a set of samples to the devision boundary in high dimensional sapce where x is series of samples of form [n_samples, n_features]
                           Attributes include:
                           support_: array of indices belonging to selected support vectors
                           support_vectors_: actual samples chosen as support vectors
                           intercept_:constants of the decision function
                           dual_coef_: each support vectors contribution to the decision function, on per classification basis.

                           KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=10,
                                   n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
                                   verbose=0)

>>> labels = kmeans.predict(df)
    >>> centroids = kmeans.cluster_centers_

    Each has a .fit() method


    Supervised Learning
    Data has to be split, one portion for machine learning and the other for testing.

    code for doing so
    >>> from sklearn.cross_validation import train_test_split
    >>> data   = [0,1,2,3,4, 5,6,7,8,9]  # input dataframe samples
    >>> labels = [0,0,0,0,0, 1,1,1,1,1]  # the function we're training is " >4 "

>>> data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.5, random_state=7)

    >>> data_train
    [9, 7, 3, 6, 4]

    >>> label_train
    [1, 1, 0, 1, 0]

    >>> data_test
    [8, 5, 0, 2, 1]

    >>> label_test
    [1, 1, 0, 0, 0]

    TO test the results of machine learnging the following can be bone;

    from sklearn.metrics import accuracy_score

# Returns an array of predictions:
>>> predictions = my_model.predict(data_test) 
    >>> predictions
    [0, 0, 0, 1, 0]

# The actual answers:
    >>> label_train
    [1, 1, 0, 1, 0]

>>> accuracy_score(label_train, predictions)
    0.59999999999999998

>>> accuracy_score(label_train, predictions, normalize=False)
    3
    Unsupervised methods learnt:

    FOr reducing features:
    1. PCA - features with linear rlshps
    2. Isomap - features with non linea rlshps

    For clustering
    1. K-means

    Using Kmeans
    >>> from sklearn.cluster import KMeans
    >>> kmeans = KMeans(n_clusters=5)
>>> kmeans.fit(df)
    KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=5, n_init=10,
            n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
            verbose=0)

>>> labels = kmeans.predict(df)
    >>> centroids = kmeans.cluster_centers_

    Each has a .fit() method


    Supervised Learning
    Data has to be split, one portion for machine learning and the other for testing.

    code for doing so
    >>> from sklearn.cross_validation import train_test_split
    >>> data   = [0,1,2,3,4, 5,6,7,8,9]  # input dataframe samples
    >>> labels = [0,0,0,0,0, 1,1,1,1,1]  # the function we're training is " >4 "

>>> data_train, data_test, label_train, label_test = train_test_split(data, labels, test_size=0.5, random_state=7)

    >>> data_train
    [9, 7, 3, 6, 4]

    >>> label_train
    [1, 1, 0, 1, 0]

    >>> data_test
    [8, 5, 0, 2, 1]

    >>> label_test
    [1, 1, 0, 0, 0]

    TO test the results of machine learnging the following can be bone;

    from sklearn.metrics import accuracy_score

# Returns an array of predictions:
>>> predictions = my_model.predict(data_test) 
    >>> predictions
    [0, 0, 0, 1, 0]

# The actual answers:
    >>> label_train
    [1, 1, 0, 1, 0]

>>> accuracy_score(label_train, predictions)
    0.59999999999999998

>>> accuracy_score(label_train, predictions, normalize=False)
    3
    K Nearest Neighbor:
    Stores all the samples and in testing, finds the K nearest neighbors.
    Works only on numerical features. Bag of words can be used to numerize.
    Once neighbors are found, K-Neighbours looks at their classes and votes to select the best fit.
    Decision boundaries:can take different shapes
    the higher the K value the smoother and less jittery the decision surface

    Before starting out, separate labels from the data features:
    X = pd.read_csv('data.set', index_col=0)
# Immediately copy out the classification / label / class / answer column
    y = X['classification'].copy()
    X.drop(labels=['classification'], inplace=True, axis=1)

    The KNeighborsClassifier class constructor takes in a few arguments, most optional:

    n_neighbors: The number of neighbors to consider. Keep it odd when doing binary classification, particularly when you use uniform weighting.
    weights: How to count the votes from the neighbors; does everyone get an equal vote, a weighted vote, or something else?
    algorithm: You can select an optimization method for searching through your training data set to find the nearest neighbors.

    And here is the algorithm in action:

    >>> # From now on, you only train on a "portion" of your dset:
>>> X_train = pd.DataFrame([ [0], [1], [2], [3] ])
    >>> y_train = [0, 0, 1, 1]

    >>> from sklearn.neighbors import KNeighborsClassifier
    >>> model = KNeighborsClassifier(n_neighbors=3)
    >>> model.fit(X_train, y_train) 
KNeighborsClassifier(...)

    >>> # You can pass in a dframe or an ndarray
>>> model.predict([[1.1]])

>>> model.predict_proba([[0.9]])
    [[ 0.66666667  0.33333333]]

    Gotchas: Data nas to be measurable
    Sensitive to feature scaling
    Care shoudl be taken with overall class distribution eg. 30%A and 70%B might lead to preference for B


    Linear Regression
    It implements:
fit(), predict(), fit_predict(), score()
    Outputs include:
    intercept_: the scalar constant offset value
    coef_: array of weights


    To model a dataset using linear regression, use the following code:

    >>> from sklearn import linear_model
    >>> model = linear_model.LinearRegression()
>>> model.fit(X_train, y_train)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)

    Data to be separated.
    x_train: singe dimensional array, shaped like [n_samples] where each sample correspondss to the single feature being fit. You can rum multiple linear regression models simultaneously on same dataset by passing in [n_samplse, n_features] array.

    y_train: it can be [n_samples] or [n_samples, n_targets]

    .score(): returns R2 coeff. that shows how good you model fits your dat        >>> # R2 Score
              >>> model.score(X_test, y_test)
              153.244939109

    >>> # Sum of Squared Distances
>>> np.sum(model.predict(X_test) - y_test) ** 2)
    5465.15

    R2 increases with more features, thus be careful with features chosen otherwise overfitting occurs.

    It can also work with categorical data, using dummy boolean columns.Increase no of targets, or training label dimension, and each calculation will have its offset stores in intercepts_ array, and coeff_attribute becomes array of arrays, one per target.(Multi-Output Linear Regression).
    Gotchas:
    works only on linear rlshps
    cant provide some values since it works on mean
    assumes variables are linearly independent


    SVC
    Supporot Vector Machines are a set of supervsed lean. algorithms that can be used for class., regre. and outlier detection.
    SVC- Support Vector Classifier
    Used when speed is required.
    SVC is not probailistic. Classification is calculated based off the geometry of your dataset.


    Usage:
    Important params are:
    kernel : type of kernel used. Default is radial basis function rbf. Others supported include: linear, poly, sigmoid, precomputeed.
    Of the many configurable parameters for SciKit-Learn's svm.SVC class, the most important three in order are:
    C: penalty parameter for the error term. Lower C valuew, smoother and more generalzed the decision boundary.
    gamma: inversely proportional to extent single training sample influence extends.

    To get started with SVC, import it as usual:

    >>> from sklearn.svm import SVC
    >>> model = SVC(kernel='linear')
>>> model.fit(X, y) 
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
            decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
            max_iter=-1, probability=False, random_state=None, shrinking=True,
            tol=0.001, verbose=False)

It has .fit(), .predict(), .score()
    .decision_function(x): calculates the distance of a set of samples to the devision boundary in high dimensional sapce where x is series of samples of form [n_samples, n_features]
                           Attributes include:
                           support_: array of indices belonging to selected support vectors
                           support_vectors_: actual samples chosen as support vectors
                           intercept_:constants of the decision function
                           dual_coef_: each support vectors contribution to the decision function, on per classification basis.


                           DECISION TREES
                           Has one root and mutple brances each dealing with a particular decisiono
                           They are indifferent to feature scaling
                           Usage3:
                           >>> from sklearn import tree
    >>> model = tree.DecisionTreeClassifier(max_depth=9, criterion="entropy")
>>> model.fit(X,y)
    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=9,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')

# .DOT files can be rendered to .PNGs, if you've already `brew install graphviz`.
    >>> tree.export_graphviz(model.tree_, out_file='tree.dot', feature_names=X.columns)

    >>> from subprocess import call
    >>> call(['dot', '-T', 'png', 'tree.dot', '-o', 'tree.png'])


    SciKit-Learn's trees are quite configurable:

    criterion By default, SciKit-Learn uses Gini, which is an impurity rating. Alternatively, you could also make use of information gain, or entropy instead.
    splitter Lets you control of the algorithm chooses the best split or not. We'll discuss why that's importance once you move to random forest classifier.
    max_features One of the possible splitter options for splitter above is called 'best'. SciKit-Learn runs a bunch of tests on your features to figure out which mechanism should be used when searching for the best split. This parameter limits the number of features to consider while doing this.

    You can get :end-node, leaf classifications, entire tree.
    Feature_importances vector can be got.

    RANDOM FOREST
    many decision trees to solve problem

    Divides training set into train/test split. Each forest randomly samples from training set, leading to trees existing in their independent subspace. This is: tree bagging/bootstrap aggregation

    Fringe results get blurred out due to averaging effect of multiple trees.
    ALso does feature bagging ie. randomly selecting diff. features at each node brancksplit
    Prediction made using majority vote label
    Our of bag samples are ethose withheld from individual tree during training

    Some of the new, optional parameters you can pass in while instantiating your model include:

    n_estimators Controls the density of the forest ensemble.
    bootstrap Also known as bagging. Every trained tree is grown using an independently drawn subset of your input data. As such, training samples not used for training an individual tree are considered out-of-bag for that one tree.
    obb_score Controls whether to use out-of-bag samples to estimate a generalization error. By default, this is turned off, with the assumption that you'll be using random forest just like any other SciKit-Learn estimator, and handling the splitting of your training/testing data manually, along with its scoring.

    The code to get a new instance of the RandomForestClassifier up and running in your projects looks like this:

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> model = RandomForestClassifier(n_estimators=10, oob_score=True)
>>> model.fit(X, y)
    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=True, random_state=None, verbose=0, warm_start=False)

    >>> print model.oob_score_
    0.789925345kk
    .estimators_attribute: structure on individual decision trees
    if oob_score=True and there are enough trees and data samples: .obb_score_ attribute will have calculated an accuracy metric. 

    Gotchas:
    Two drawbacks of random forest are that since you're now planting an entire forest as opposed to a single tree, both training and prediction execution times suffer tremendously. Particularly, training is an order of magnitude more time consuming. You also lose the ability to inspect the resulting structure of your classifier as easily. No longer can you just print out a .dot-file flow chart since no single flowchart encompasses the forest, and your results aren't strictly based on following a single logic diagram. Along the same lines, the forest can no longer be linearized into IF...THEN blocks anymore.

    COnfusion
    Choosing the right Estimator
    Model: Formula/algorithm desinged to reprsent the mechanics of data
    Estimators
    Predictors:extimators designed to deal with data that has attributes to be predicted

    The algorithm choice is dictated by:
    dimensionality of the data
    geomtric natur
    types of fetures used to rep data
    no of training samples
    required training and prediction speeds
    predictive accuracy required
    How configurable the model should be

    COnfusion Matrix
    It displays the predicted outputs against the tru observed outputs
    Helps coz it helps engineer additional features to identify confusing results
    If we have:
    >>> import sklearn.metrics as metrics
    >>> y_true = [1, 1, 2, 2, 3, 3]  # Actual, observed testing dataset values
    >>> y_pred = [1, 1, 1, 3, 2, 3]  # Predicted values from your model
    To compute confusion matrix then:
    >>> metrics.confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
        [1, 0, 1],
        [0, 1, 1]])

    >>> import matplotlib.pyplot as plt

    >>> columns = ['Cat', 'Dog', 'Monkey']
>>> confusion = metrics.confusion_matrix(y_true, y_pred)

    Use imshow to view confusion matrix
    >>> plt.imshow(confusion, cmap=plt.cm.Blues, interpolation='nearest')
    >>> plt.xticks([0,1,2], columns, rotation='vertical')
    >>> plt.yticks([0,1,2], columns)
>>> plt.colorbar()

>>> plt.show()

    Cross Validation
    Scoring
    Simplest: scoring on the same data used during training
    Next step, ability to get correct results on unseen data
    Other metrics are: true positive, true negative, falsse positive, false negative
    GIven this data:
    >>> import sklearn.metrics as metrics
    >>> y_true = [1, 1, 2, 2, 3, 3]  # Actual, observed testing datset values
    >>> y_pred = [1, 1, 1, 3, 2, 3]  # Predicted values from your model

    To get accuracy score:
>>> metrics.accuracy_score(y_true, y_pred)
    0.5
>>> metrics.accuracy_score(y_true, y_pred, normalize=False)
    3

    Others include 
    recall_score: metrics.recall_score(y_true, y_pred, average="weighted/NOne")
    precision_score: = true_positives/(true_posiives+false_positives)
    f1_score: 2 * (precision * recall) / (precision + recall)
    full report : produces all the scores
metrics.classification_report(y_true, y_pred, target_names=target_names)

    Cross Validation:
    SPlitting data can lead to:
    subset chosen for training being best oor having outliers
    splitting essentially means losing some of the training data
    when choosing config options and testing until highest score is reached, you are essentially leaking test data to algorithm
    To solve the last issure is to ohave training set, testing set and validattion test.
    sklearn uses cross_val_score
    >>> from sklearn import cross_validation as cval
    >>> cval.cross_val_score(model, X_train, y_train, cv=10)
array([ 0.93513514,  0.99453552,  0.97237569,  0.98888889,  0.96089385,
        0.98882682,  0.99441341,  0.98876404,  0.97175141,  0.96590909])

>>> cval.cross_val_score(model, X_train, y_train, cv=10).mean()
    0.97614938602520218

    Power Tuning
    GridSearchCV
    In its simplest form, GridSearchCV works by taking in an estimator, a grid of parameters you want optimized, and your cv split value

    >>> from sklearn import svm, grid_search, datasets

>>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 5, 10]}

>>> model = svm.SVC()

    >>> classifier = grid_search.GridSearchCV(model, parameters)
>>> classifier.fit(iris.data, iris.target)
    GridSearchCV(cv=None, error_score='raise',
            estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
                decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
                max_iter=-1, probability=False, random_state=None, shrinking=True,
                tol=0.001, verbose=False),
            fit_params={}, iid=True, n_jobs=1,
            param_grid={'kernel': ('linear', 'rbf'), 'C': [1, 5, 10]},

            pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)

RandomizedSearchCV:
perfomrs multiple grid optimizations consecutively

>>> parameter_dist = {
  'C': scipy.stats.expon(scale=100),
    'kernel': ['linear'],
      'gamma': scipy.stats.expon(scale=.1),
      }

      >>> classifier = grid_search.RandomizedSearchCV(model, parameter_dist)
      >>> classifier.fit(iris.data, iris.target)

      RandomizedSearchCV(cv=None, error_score='raise',
                estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
                  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
                    max_iter=-1, probability=False, random_state=None, shrinking=True,
                      tol=0.001, verbose=False),
                                fit_params={}, iid=True, n_iter=10, n_jobs=1,
                                          param_distributions={'kernel': ['linear'], 'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x110345c50>, 'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x110345d90>},
                                                    pre_dispatch='2*n_jobs', random_state=None, refit=True,
                                                              scoring=None, verbose=0)

Pipelining
